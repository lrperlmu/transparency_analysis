---
title: "Transparency Analysis Code Base"
author: "Eric Kernfeld"
output: html_document
---

This document defines functions to carry out the analysis and diagnostics outlined elsewhere in the repo. It is intended to be called via `transparency_analysis_script`, which sets the working directory correctly. For more information, check out the README and then contact Eric Kernfeld, `ekernf01 (obvious symbol) uw (department of transportation) edu`.

###Setting up enviroment

```{r, results='hide', message = FALSE, warning = FALSE}
require(lme4)
require(car)
require(assertthat)
require(xtable)
require(ggplot2)
```

###Data reshaping

####Participant ids
In the file `csv/README.md` from Leah's data, "Odd-numbered participants, except #5, have (Baseline | Monitor | Oculus). Even-numbered participants, and #5, have (Baseline | Oculus | Monitor)." The following code separates the two sets of participants.

```{r}
ids = read.table("../data/participant_ids.csv")$V1
ids_monitor_first = ids[ids != 5 & ids %% 2 == 1]
ids_oculus_first = ids[ids == 5 | ids %% 2 == 0]
print(ids_monitor_first)
print(ids_oculus_first)
```

####Assembling responses and covariates

#####Terminology
To clarify terminology, "response" refers to a variable measured in the experiment. "Covariates" are anything we use to explain variation in the response: participant id, variables representing the learning effect, and "condition." By "condition", I mean one of these six combinations:

- `baseline_P1`
- `baseline_P2`
- `monitor_P1`
- `monitor_P2`
- `oculus_P1`
- `oculus_P2`.

The following R function loads in the data and converts it to "long" format. 

#####Input
For the code to work properly, the files in `data/reshaped` should contain data for one response variable each. The files should be entitled:

- `language.csv`
- `pointing.csv`
- `accuracy.csv`
- `number_of_words.csv`
- `attempt_times.csv`
- `completion_times.csv`
- `number_of_attempts.csv`.

In each file, there should be one row per participant, and the first column should be the participant's id. After that, there should be one column for each condition. The first column header should be `ids` and the following six headers should match the bulleted list above. Comments should be prefaced with `#`.

#####Output
For the output, there will be one column for the measurements, one for the learning-effect indicators, one for the participant ($i$ in `analysis_plans.md`, `ids` in this code) and one for the condition ($j$ in `analysis_plans.md` and `condition` in this code). The indicator variable $X_{ij}$ from `analysis_plans.md` appears in the code as `learning_23`. The variable `learning_123` in this code is meant to equal $X_{ij} + X_{ij}'$ from `analysis_plans.md`.

```{r}
BINARY_METRICS = c("language", "pointing", "accuracy")
QUANTITATIVE_METRICS = c("number_of_words", 
                         "attempt_times", 
                         "completion_times", 
                         "number_of_attempts")
TASK_METRICS = c(BINARY_METRICS, QUANTITATIVE_METRICS)
TM_WITH_UNITS = c(paste0(BINARY_METRICS, " (Odds)"),
                  paste0(QUANTITATIVE_METRICS, c("", " (sec)", " (sec)", "")))
TM_WITH_EFFECT_UNITS = c(paste0(BINARY_METRICS, " (% change in odds)"),
                         paste0(QUANTITATIVE_METRICS, " (% change)"))
CONDITION_NAMES = c("baseline_P1", "baseline_P2", 
                     "monitor_P1",  "monitor_P2", 
                      "oculus_P1",   "oculus_P2")
CONDITION_NAMES_SHORT = c("B_P1", "B_P2", 
                          "M_P1", "M_P2", 
                          "O_P1", "O_P2")

get_long_format_data = function(task_metric){
  #load, reshape, and give meaningful names
  fname = paste0("../data/reshaped/", task_metric, ".csv")
  
  wide_data = read.csv(file = fname, comment.char = "#")
  assertthat::assert_that(all( names(wide_data) == c("participant_id", CONDITION_NAMES) ))
  long_data = reshape2::melt(wide_data, id.vars = "participant_id") 
  long_data_nice = data.frame(task_metric = numeric(120))
  long_data_nice$task_metric = long_data$value
  long_data_nice$condition = long_data$variable
  long_data_nice$ids = long_data$participant_id
  
  #get total trials for accuracy data: 10 for P1, 5 for P2
  if(task_metric == "accuracy"){
    long_data_nice$num_commands = 
      5 *(long_data_nice$condition %in% c("baseline_P2", "monitor_P2", "oculus_P2")) +
      10*(long_data_nice$condition %in% c("baseline_P1", "monitor_P1", "oculus_P1"))
  } 
  
  #get total trials for pointing and language data
  if(task_metric %in% c("pointing", "language")){
    wide_totals = read.csv(file = paste0("../data/reshaped/", task_metric, "_totals.csv"), comment.char = "#")
    long_totals = reshape2::melt(wide_totals, id.vars = "participant_id") 
    long_data_nice$num_commands = long_totals$value
  } 
  
  #get logs for non-binary data
  if (!task_metric %in% BINARY_METRICS){
    long_data_nice$log_task_metric = log(long_data_nice$task_metric)
  } 

  # Xij = 1 if j = 3, 4 and person i used the monitor last, oculus first
  # Xij = 1 if j = 5, 6 and person i used the oculus last, monitor first
  # 0 otherwise
  long_data_nice$learning_23 = ( long_data_nice$ids %in% ids_oculus_first ) &  
    ( long_data_nice$condition %in% c("monitor_P1", "monitor_P2") )
  long_data_nice$learning_23 = ( long_data_nice$ids %in% ids_monitor_first ) & 
    ( long_data_nice$condition %in% c("oculus_P1", "oculus_P2") )
  
  # Xij' = 1 if j = 3, 4, 5, 6
  # 0 otherwise
  X_prime_ij = long_data_nice$condition %in% c("monitor_P1", "monitor_P2", "oculus_P1", "oculus_P2")
  long_data_nice$learning_123 = X_prime_ij + long_data_nice$learning_23
  
  return(long_data_nice)
}
```

###Analysis and diagnostic function definitions

This function performs calculations for either of the analyses (specify just an integer, 1 or 2) on any task metric (specify a string matching one of the filenames from above, but without the `.csv`.) For example, to perform Analysis 1 on the attempt times, use the command 

    fitted_model = print_short_results(task_metric = "attempt_times", which_analysis = 1)

. This prints some test results as a side effect, and it returns a fitted model object.

```{r}
fit_reg = function(task_metric, which_analysis){

  # specify form of model, varying learning effect based on which analysis 
  # we're doing and response based on binary versus continuous
  assertthat::assert_that(which_analysis %in% 1:2)
  if(which_analysis==1){
    if(task_metric %in% BINARY_METRICS){
      my_formula = cbind(task_metric, num_commands - task_metric) ~ (1|ids) + 0 + condition + learning_23
    } else {
      my_formula =log_task_metric ~ (1|ids) + 0 + condition + learning_23
    }
  } 
  if(which_analysis==2){
    if(task_metric %in% BINARY_METRICS){
      my_formula = cbind(task_metric, num_commands - task_metric) ~ (1|ids) + 0 + condition + learning_123
    } else {
      my_formula = log_task_metric ~ (1|ids) + 0 + condition + learning_123
    }
  }   
  
  long_data = get_long_format_data(task_metric)
  
  # call either linear regression (non-binary) or binomial regression (binary)
  if(task_metric %in% BINARY_METRICS){
    fitted_mod = lme4::glmer(data = long_data, formula = my_formula, 
                            family = binomial())
  } else{
    fitted_mod = lme4::lmer (data = long_data, formula = my_formula)
  }
  
  return(fitted_mod)
}
```

This function creates diagnostic plots:

- fitted values versus residual magnitudes to check homoskedasticity
- participants versus residuals to check for serial correlation
- residual histograms and random-effect estimates to check for skew

```{r}
make_diagnostic_plots = function(task_metric, which_analysis){
  fitted_model = fit_reg(task_metric, which_analysis)
  long_data = fitted_model@frame
  long_data$res = resid(fitted_model)
  long_data$fitted_vals = fitted(fitted_model)
  p1 = ggplot(long_data) + 
    geom_point(aes(y = res, x = fitted_vals)) +
    ggtitle("Rsdl ~ fit values")
  p2 = ggplot(long_data) + 
    geom_boxplot(aes(y = res, x = ids, group = ids)) +
    ggtitle("Rsdl ~ participant")
  p3 = ggplot(long_data, aes(x = res)) + 
    geom_density() + 
    ggtitle("Rsdl")
  print(p1); print(p2); print(p3)
  cat("  \n")
}
```

This function extracts p-values from the fitted model objects generated by functions defined above.

```{r}
HYPOTHESIS_NAMES = c("H1_transp_effects", 
                     "H2_transp_after_effects", 
                     "H3_headset_v_monitor",
                     "H3_headset_v_monitor_aftereffect")
get_pvals = function(task_metric, which_analysis){

  fitted_model = fit_reg(task_metric, which_analysis)
  
  #Set up name/contrast corresponding to each test

  the_tests = as.list(HYPOTHESIS_NAMES); names(the_tests) = HYPOTHESIS_NAMES
  #mu1 == mu3 == mu5
  the_tests[[1]] = list(contrast_vector = rbind(c(1, 0, -1,  0,  0,  0, 0),
                                                c(1, 0,  0,  0, -1,  0, 0)))
  #mu2 == mu4 == mu6
  the_tests[[2]] = list(contrast_vector = rbind(c(0, 1,  0, -1,  0,  0, 0),
                                                c(0, 1,  0,  0,  0, -1, 0)))
  #mu3 == mu5
  the_tests[[3]] = list(contrast_vector = rbind(c(0, 0,  1,  0, -1,  0, 0)))
  
  #mu4 == mu6
  the_tests[[4]] = list(contrast_vector = rbind(c(0, 0,  0,  1,  0, -1, 0)))

  #Exclude some tests
  if(which_analysis == 1){ tests_to_do = 3:4 }
  if(which_analysis == 2){ tests_to_do = 1:2 }
  the_tests = the_tests[tests_to_do]
  
  for(test_ind in seq_along(the_tests)){
    contrast_temp = the_tests[[test_ind]]$contrast_vector
    test_results = car::linearHypothesis(fitted_model, hypothesis.matrix = contrast_temp)
    pval_temp = test_results$`Pr(>Chisq)`[2]
    the_tests[[test_ind]]$pval = pval_temp
    #es_temp = sum(contrast_temp * fitted_model@beta)
    #the_tests[[test_ind]]$effect_size = es_temp
    #the_tests[[test_ind]]$effect_size_se = abs( es_temp / qnorm(pval_temp/2))
  }
  return(the_tests)
}
```

This function loops through all the task metrics, hypotheses, and analyses, printing Markdown text and pvalue tables (in HTML) as a side-effect.

```{r}
print_tests_A1A2 = function(){
  #Both analyses
  for(i in 1:2){
    cat(paste0("####Analysis ", i)[1])
    
    my_table = data.frame(matrix(NA, nrow = length(TASK_METRICS), 
                                 ncol = length(HYPOTHESIS_NAMES)))
    names(my_table) = HYPOTHESIS_NAMES
    row.names(my_table) = TASK_METRICS

    #all task metrics
    for(j in seq_along(TASK_METRICS)){
      test_results_temp = get_pvals(TASK_METRICS[j], which_analysis = i)
      for(test_name in names(test_results_temp)){
        my_table[j,test_name] = test_results_temp[[test_name]]$pval
      }
    }
    print(xtable::xtable(my_table, digits = 4), type = "html")
  }
}
```

This function loops through all the models, printing diagnostic plots and Markdown text.
```{r}
print_diagnostics = function(){
  for(i in 1:2){
    cat("  \n  \n")
    cat(paste("####Analysis", i))
    for(task_metric in TASK_METRICS){
      cat("  \n  \n")
      cat("#####Task metric", task_metric, "  \n")
      make_diagnostic_plots(task_metric, i)
    }
  }
}
```

This function loops through all the task metrics, hypotheses, and analyses, printing Markdown text and effect size tables (in HTML) as a side-effect.

```{r}
get_effect_sizes = function(){
  for(i in 1:2){
    cat(paste0("####Analysis ", i)[1])
    main_colnames = paste0(CONDITION_NAMES_SHORT)
    effect_colnames = c("LEARN", 
                        "B_to_M_P1",
                        "B_to_O_P1",
                        "M_to_O_P1", 
                        "B_to_M_P2",
                        "B_to_O_P2",
                        "M_to_O_P2")
    
    main_table = matrix(NA,      
                        nrow = length(       TM_WITH_UNITS), ncol = length(main_colnames),
                        dimnames = list(qm = TM_WITH_UNITS,           cn = main_colnames)) 
    effect_table = matrix(NA,    
                          nrow = length(       TM_WITH_EFFECT_UNITS), ncol = length(effect_colnames),
                          dimnames = list(qm = TM_WITH_EFFECT_UNITS,           cn = effect_colnames)) 
    
    map_to_pct = function(x) (100 * (exp(x) - 1))
    quant_proc = function(x) map_to_pct(x)
    bin_proc = function(x) map_to_pct(x)

    for(j in seq_along(TASK_METRICS)){
      task_j = TASK_METRICS[j]
      fitted_model = fit_reg(task_j, which_analysis = i)
      #Coeffs but not in log space: odds or original units
      main_table[j, ] = exp(fitted_model@beta[1:6])

      #Differences as percents
      temp = c(fitted_model@beta[7],
               fitted_model@beta[3] - fitted_model@beta[1],
               fitted_model@beta[5] - fitted_model@beta[1],
               fitted_model@beta[5] - fitted_model@beta[3],
               fitted_model@beta[4] - fitted_model@beta[2],
               fitted_model@beta[6] - fitted_model@beta[2],
               fitted_model@beta[6] - fitted_model@beta[4])
      if(task_j %in% QUANTITATIVE_METRICS){effect_table[j, ] = quant_proc(temp)}
      if(task_j %in% BINARY_METRICS)      {effect_table[j, ] = bin_proc(temp)}
    }
    cat(paste0("  \n#####Predictions by condition  \n"))
    print(xtable::xtable(main_table), type = "html")
    cat(paste0("  \n#####Predicted effects from changing between conditions \n"))
    print(xtable::xtable(effect_table), type = "html")
  }
}
```