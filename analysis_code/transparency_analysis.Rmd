---
title: "Transparency Analysis Code"
author: "Eric Kernfeld"
output: html_document
---

This document carries out the analysis and diagnostics outlined elsewhere in the repo (please read `analysis_plans.md` first). For more information, contact Eric Kernfeld, `ekernf01 (obvious symbol) uw (department of transportation) edu`.

###Setting up enviroment

```{r, results='hide', warning=FALSE, message=FALSE}
rm(list = ls()) #clear namespace
#This needs to be changed depending on where you have your clone of the repository.
PATH_TO_THIS_FILE = "~/Desktop/winter_2016/consulting/Leah Perlmutter (cs)/transparency_analysis/analysis_code/"
setwd(PATH_TO_THIS_FILE)
require(lme4)
require(lmerTest)
require(assertthat)
```

###Data reshaping

####Participant ids
In the file `csv/README.md` from Leah's data, "Odd-numbered participants, except #5, have (Baseline | Monitor | Oculus). Even-numbered participants, and #5, have (Baseline | Oculus | Monitor)." The following code separates the two sets of participants.

```{r}
ids = read.table("../data/participant_ids.csv")$V1
ids_monitor_first = ids[ids != 5 & ids %% 2 == 1]
ids_oculus_first = ids[ids == 5 | ids %% 2 == 0]
print(ids_monitor_first)
print(ids_oculus_first)
```

####Response variables and covariates

#####Terminology
To clarify terminology, "response" refers to a variable measured in the experiment. "Covariates" are anything we use to explain variation in the response: participant id, variables representing the learning effect, and "condition." By "condition", I mean one of these six combinations:

- `baseline_P1`
- `baseline_P2`
- `monitor_P1`
- `monitor_P2`
- `oculus_P1`
- `oculus_P2`.

The following R function loads in the data and converts it to "long" format. 

#####Input
For the code to work properly, the files in `data/response` should contain data for one response variable each. The files should be entitled:

- `language.csv`
- `pointing.csv`
- `accuracy.csv`
- `number_of_words.csv`
- `attempt_times.csv`
- `number_of_attempts.csv`.

In each file, there should be one row per participant, and the first column should be the participant's id. After that, there should be one column for each condition. The first column header should be `ids` and the following six headers should match the bulleted list above.

#####Output
As for the output, there will be one column for the measurements, one for the learning-effect indicators, one for the participant ($i$ in `analysis_plans.md`, `ids` in this code) and one for the condition ($j$ in `analysis_plans.md` and `condition` in this code). The indicator variable $X_{ij}$ from `analysis_plans.md` appears in the code as `learning_23`. The variable `learning_123` in this code is meant to equal $X_{ij} + X_{ij}'$ from `analysis_plans.md`.

```{r}
binary_metrics = c("language", "pointing", "accuracy")
task_metrics = c(binary_metrics, 
                 "number_of_words", 
                 "attempt_times", 
                 "number_of_attempts")
condition_names = c("baseline_P1", "baseline_P2", 
                     "monitor_P1",  "monitor_P2", 
                      "oculus_P1",   "oculus_P2")

get_long_format_data = function(task_metric){
  #load, reshape, and give meaningful names
  wide_data = read.csv(file = paste0("../data/response/", task_metric, ".csv"), comment.char = "#")
  assertthat::assert_that(all( names(wide_data) == c("ids", condition_names) ))
  long_data = reshape2::melt(wide_data, id.vars = "ids") 
  long_data_nice = data.frame(numeric(120))
  long_data_nice$task_metric = long_data$value
  long_data_nice$condition = long_data$variable
  
  #get total trials for binary and log responses for non-binary
  if(task_metric %in% binary_metrics ){
    long_data_nice$num_actions = 
      5 *(long_data_nice$condition %in% c("baseline_P2", "monitor_P2", "oculus_P2")) +
      10*(long_data_nice$condition %in% c("baseline_P1", "monitor_P1", "oculus_P1"))
  } else {
    long_data_nice$log_task_metric = log(long_data_nice$task_metric)
  } 

  # Xij = 1 if j = 3, 4 and person i used the monitor last, oculus first
  # Xij = 1 if j = 5, 6 and person i used the oculus last, monitor first
  # 0 otherwise
  long_data_nice$learning_23 = ( long_data_nice$ids %in% ids_oculus_first ) &  
    ( long_data_nice$condition %in% c("monitor_P1", "monitor_P2") )
  long_data_nice$learning_23 = ( long_data_nice$ids %in% ids_monitor_first ) & 
    ( long_data_nice$condition %in% c("oculus_P1", "oculus_P2") )
  
  # Xij' = 1 if j = 3, 4, 5, 6
  # 0 otherwise
  X_prime_ij = long_data_nice$condition %in% c("monitor_P1", "monitor_P2", "oculus_P1", "oculus_P2")
  long_data_nice$learning_123 = X_prime_ij + long_data_nice$learning_23
  
  return(long_data_nice)
}
```

###Analysis code

This function performs calculations for either of the analyses (specify just an integer, 1 or 2) on any task metric (specify a string matching one of the filenames from above, but without the `.csv`.) For example, to perform Analysis 1 on the attempt times, use the command 

    fitted_model = fit_and_test(task_metric = "attempt_times", which_analysis = 1)
.

```{r}

fit_and_test = function(task_metric, which_analysis){
  
  # We model the log task metric for non-binary data and 
  # the "successes" versus "failures" for binary data
  if(task_metric %in% binary_metrics){
    response_formula = ~ cbind(task_metric, num_actions - task_metric)
  } else{
    response_formula = ~ log_task_metric
  }
  
  # specify form of model, varying learning effect based on which analysis we're doing
  assertthat::assert_that(which_analysis %in% 1:2)
  if(which_analysis==1){
    my_formula = response_formula ~ (1|ids) + condition + learning_23
  } 
  if(which_analysis==2){
    my_formula = response_formula ~ (1|ids) + condition + learning_123
  }   
  
  long_data = get_long_format_data(task_metric)
  
  # call either linear regression (non-binary) or quasi-binomial regression (binary)
  if(task_metric %in% binary_metrics){
    fitted_mod = lme4::glmer(data = long_data, formula = my_formula, 
                            family = quasibinomial())
  } else{
    fitted_mod = lme4::lmer (data = long_data, formula = my_formula)
  }
  
  print("Model summary with p-values:")
  lmerTest::summary(fitted_mod)
  return(fitted_mod)
}
```
